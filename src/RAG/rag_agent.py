"""
RAG Agent for Retrieval-Augmented Generation.

Main agent that combines vector search with LLM generation:
- Inherits from BaseAgent for consistency
- Performs vector/hybrid search for relevant documents
- Builds context from search results
- Generates responses using LLM with retrieved context
- Maintains user isolation and error handling
"""

from typing import List, Dict, Any, Optional
from src.core.base_agent import BaseAgent
from src.core.types import AgentState
from src.llms.llm_factory import LLMFactory
from .vector_search import VectorSearchService
from .context_builder import ContextBuilder


class RAGAgent(BaseAgent):
    """RAG Agent that combines retrieval and generation."""
    
    def __init__(self, 
                 llm_factory: LLMFactory,
                 search_type: str = "hybrid",
                 max_context_length: int = 4000,
                 max_search_results: int = 10):
        """
        Initialize RAG agent.
        
        Args:
            llm_factory: LLM factory for creating LLM instances
            search_type: Type of search ("dense", "sparse", "hybrid")
            max_context_length: Maximum context length for LLM
            max_search_results: Maximum number of search results
        """
        super().__init__(llm_factory)
        
        self.search_type = search_type
        self.max_search_results = max_search_results
        
        # Initialize RAG components
        try:
            # Get embedding provider from file embedding service
            from src.services.file_embedding_service import get_file_embedding_service
            embedding_service = get_file_embedding_service()

            # Initialize search service with proper embedding provider
            self.search_service = VectorSearchService(embedding_service.embedding_provider)
            self.context_builder = ContextBuilder(max_context_length=max_context_length)
            self.rag_available = True
            print("‚úÖ RAG Agent initialized successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è RAG components not available: {e}")
            self.search_service = None
            self.context_builder = None
            self.rag_available = False
    
    def get_agent_name(self) -> str:
        """Return the name of this agent."""
        return "RAGAgent"
    
    def get_prompt(self, user_input: str, context: list = None) -> str:
        """
        Generate prompt with RAG context.
        This method is called by BaseAgent.process()
        
        Args:
            user_input: User query
            context: Conversation context (not used for RAG context)
            
        Returns:
            Prompt with retrieved context
        """
        # If RAG not available, use simple prompt
        if not self.rag_available:
            return self._get_fallback_prompt(user_input, context)
        
        # Get user_id from state (will be set by process method)
        user_id = getattr(self, '_current_user_id', 'default_user')
        
        # Perform RAG retrieval
        rag_context = self._retrieve_context(user_input, user_id)
        
        # Build prompt with context
        return self._build_rag_prompt(user_input, rag_context, context)
    
    def _retrieve_context(self, query: str, user_id: str) -> Dict[str, Any]:
        """
        Retrieve relevant context for the query.
        
        Args:
            query: User query
            user_id: User ID for isolation
            
        Returns:
            Context data with documents and metadata
        """
        try:
            # Perform vector search
            search_results = self.search_service.search(
                query=query,
                user_id=user_id,
                search_type=self.search_type,
                limit=self.max_search_results
            )
            
            # Build context from results
            context_data = self.context_builder.build_context(search_results, query)
            
            print(f"üìö Retrieved {context_data['total_documents']} documents for query")
            return context_data
            
        except Exception as e:
            print(f"‚ùå Context retrieval failed: {e}")
            return {
                "context": "",
                "documents": [],
                "sources": [],
                "total_documents": 0,
                "context_length": 0
            }
    
    def _build_rag_prompt(self, 
                         user_input: str, 
                         rag_context: Dict[str, Any],
                         conversation_context: list = None) -> str:
        """
        Build prompt with RAG context.
        
        Args:
            user_input: User query
            rag_context: Retrieved context data
            conversation_context: Previous conversation
            
        Returns:
            Complete prompt with context
        """
        prompt_parts = []
        
        # System instruction
        prompt_parts.append("""B·∫°n l√† m·ªôt AI assistant th√¥ng minh v√† h·ªØu √≠ch. B·∫°n c√≥ kh·∫£ nƒÉng:

1. **Tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu**: S·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt
2. **Tr√≠ch d·∫´n ngu·ªìn**: Lu√¥n ƒë·ªÅ c·∫≠p ƒë·∫øn ngu·ªìn th√¥ng tin khi tr·∫£ l·ªùi
3. **Ph√¢n t√≠ch v√† t·ªïng h·ª£p**: K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu t√†i li·ªáu ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán
4. **Th·ª´a nh·∫≠n gi·ªõi h·∫°n**: N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, h√£y th√†nh th·∫≠t n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ ƒë·ªß th√¥ng tin

**Nguy√™n t·∫Øc quan tr·ªçng:**
- ∆Øu ti√™n th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu kh√°c
- Cung c·∫•p c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c v√† d·ªÖ hi·ªÉu
- ƒê·ªÅ c·∫≠p ngu·ªìn t√†i li·ªáu khi c√≥ th·ªÉ""")
        
        # Add RAG context if available
        if rag_context["context"]:
            prompt_parts.append(f"\n{rag_context['context']}")
        else:
            prompt_parts.append("\nKh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu.")
        
        # Add conversation context if available
        if conversation_context:
            prompt_parts.append("\n--- L·ªãch s·ª≠ h·ªôi tho·∫°i ---")
            for msg in conversation_context[-3:]:  # Last 3 messages
                role = msg.get("role", "user")
                content = msg.get("content", "")
                prompt_parts.append(f"{role}: {content}")
        
        # Add current query
        prompt_parts.append(f"\n--- C√¢u h·ªèi hi·ªán t·∫°i ---")
        prompt_parts.append(f"Ng∆∞·ªùi d√πng: {user_input}")
        
        # Add instruction for response
        if rag_context["total_documents"] > 0:
            prompt_parts.append(f"\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n {rag_context['total_documents']} t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p ·ªü tr√™n. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ v√† ƒë∆∞a ra g·ª£i √Ω n·∫øu c√≥ th·ªÉ.")
        else:
            prompt_parts.append(f"\nKh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan. H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n v√† th√¥ng b√°o r·∫±ng c√¢u tr·∫£ l·ªùi kh√¥ng d·ª±a tr√™n t√†i li·ªáu c·ª• th·ªÉ n√†o.")
        
        return "\n".join(prompt_parts)
    
    def _get_fallback_prompt(self, user_input: str, context: list = None) -> str:
        """
        Generate fallback prompt when RAG is not available.
        
        Args:
            user_input: User query
            context: Conversation context
            
        Returns:
            Simple prompt without RAG context
        """
        prompt_parts = []
        
        prompt_parts.append("""B·∫°n l√† m·ªôt AI assistant th√¥ng minh v√† h·ªØu √≠ch. 
        
‚ö†Ô∏è L∆∞u √Ω: H·ªá th·ªëng t√¨m ki·∫øm t√†i li·ªáu hi·ªán kh√¥ng kh·∫£ d·ª•ng, v√¨ v·∫≠y t√¥i s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.
        
H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch h·ªØu √≠ch v√† ch√≠nh x√°c nh·∫•t c√≥ th·ªÉ.""")
        
        # Add conversation context if available
        if context:
            prompt_parts.append("\n--- L·ªãch s·ª≠ h·ªôi tho·∫°i ---")
            for msg in context[-3:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                prompt_parts.append(f"{role}: {content}")
        
        prompt_parts.append(f"\nC√¢u h·ªèi: {user_input}")
        
        return "\n".join(prompt_parts)
    
    def process(self, state: AgentState) -> AgentState:
        """
        Process the input with RAG capabilities.
        Enhanced version of BaseAgent.process() with RAG context.
        
        Args:
            state: Agent state containing input and metadata
            
        Returns:
            Updated state with RAG response
        """
        try:
            user_input = state["input"]
            user_id = state.get("user_id", "default_user")
            
            # Store user_id for use in get_prompt
            self._current_user_id = user_id
            
            # Get conversation context
            conversation_context = state.get("conversation_context", [])
            
            # Generate prompt (this will trigger RAG retrieval)
            prompt = self.get_prompt(user_input, conversation_context)
            
            # Get LLM response
            from langchain.schema import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Update state with results
            state["final_result"] = response.content
            state["errors"] = None
            
            # Add RAG metadata if available
            if self.rag_available:
                rag_context = self._retrieve_context(user_input, user_id)
                state["rag_metadata"] = {
                    "search_type": self.search_type,
                    "documents_found": rag_context["total_documents"],
                    "sources": rag_context["sources"],
                    "context_length": rag_context["context_length"]
                }
            
            print(f"‚úÖ RAG Agent processed query successfully")
            
        except Exception as e:
            error_msg = f"Error in {self.get_agent_name()}: {str(e)}"
            state["final_result"] = None
            state["errors"] = [error_msg]
            print(f"‚ùå {error_msg}")
        
        return state
