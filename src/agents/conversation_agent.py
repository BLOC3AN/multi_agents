"""
Single conversation agent that handles all types of user requests.
This agent replaces the multi-agent system with a unified approach.
"""
from src.core.base_agent import BaseAgent
from src.core.types import AgentState
from src.llms.llm_factory import LLMFactory


class ConversationAgent(BaseAgent):
    """Unified agent that handles all types of conversations and requests."""

    def get_agent_name(self) -> str:
        """Return the name of this agent."""
        return "ConversationAgent"

    def get_prompt(self, user_input: str, context: list = None) -> str:
        """Generate an intelligent prompt that can handle all types of requests."""
        
        # Base system prompt that combines all capabilities
        prompt = """Báº¡n lÃ  má»™t AI assistant thÃ´ng minh vÃ  há»¯u Ã­ch. Báº¡n cÃ³ thá»ƒ:

ðŸ§® **Giáº£i toÃ¡n**: Giáº£i cÃ¡c bÃ i toÃ¡n, phÆ°Æ¡ng trÃ¬nh, tÃ­nh toÃ¡n sá»‘ há»c má»™t cÃ¡ch chi tiáº¿t
ðŸ“š **Giáº£i thÃ­ch khÃ¡i niá»‡m**: Tráº£ lá»i cÃ¢u há»i, giáº£i thÃ­ch khÃ¡i niá»‡m, cung cáº¥p thÃ´ng tin há»¯u Ã­ch
ðŸŽ­ **SÃ¡ng tÃ¡c vÄƒn há»c**: Viáº¿t thÆ¡, truyá»‡n, sÃ¡ng tÃ¡c vÄƒn há»c sÃ¡ng táº¡o

HÃ£y phÃ¢n tÃ­ch yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng vÃ  Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p nháº¥t. 
Náº¿u lÃ  bÃ i toÃ¡n, hÃ£y giáº£i chi tiáº¿t tá»«ng bÆ°á»›c.
Náº¿u cáº§n giáº£i thÃ­ch, hÃ£y trÃ¬nh bÃ y rÃµ rÃ ng vÃ  dá»… hiá»ƒu.
Náº¿u lÃ  yÃªu cáº§u sÃ¡ng tÃ¡c, hÃ£y táº¡o ra ná»™i dung hay vÃ  Ã½ nghÄ©a.

"""

        # Add conversation context if available
        if context and len(context) > 0:
            prompt += "\n**Ngá»¯ cáº£nh cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³:**\n"
            for msg in context[-3:]:  # Use last 3 messages for context
                if msg.get("user_input"):
                    prompt += f"ðŸ‘¤ NgÆ°á»i dÃ¹ng: {msg['user_input']}\n"
                if msg.get("agent_response"):
                    # Truncate long responses for context
                    response = msg['agent_response']
                    if len(response) > 200:
                        response = response[:200] + "..."
                    prompt += f"ðŸ¤– Trá»£ lÃ½: {response}\n"
            prompt += "\n"

        # Add current user input
        prompt += f"**YÃªu cáº§u hiá»‡n táº¡i:**\nðŸ‘¤ NgÆ°á»i dÃ¹ng: {user_input}\n\n"
        prompt += "ðŸ¤– Trá»£ lÃ½: "

        return prompt

    def _detect_request_type(self, user_input: str) -> str:
        """
        Simple heuristic to detect request type for internal use.
        This is just for logging/debugging purposes.
        """
        text_lower = user_input.lower()
        
        # Math keywords
        math_keywords = ["solve", "equation", "calculate", "math", "=", "+", "-", "*", "/", 
                        "x +", "x -", "2x", "phÆ°Æ¡ng trÃ¬nh", "giáº£i", "tÃ­nh", "toÃ¡n"]
        if any(keyword in text_lower for keyword in math_keywords):
            return "math"
        
        # Poem keywords  
        poem_keywords = ["poem", "poetry", "verse", "thÆ¡", "bÃ i thÆ¡", "viáº¿t thÆ¡", 
                        "write a poem", "compose", "sÃ¡ng tÃ¡c"]
        if any(keyword in text_lower for keyword in poem_keywords):
            return "poem"
        
        # Default to general conversation
        return "general"

    def process(self, state: AgentState) -> AgentState:
        """
        Process the input and update the state.
        Enhanced version that includes request type detection for metadata.
        """
        try:
            user_input = state["input"]
            context = state.get("conversation_context", [])
            
            # Detect request type for metadata (optional)
            request_type = self._detect_request_type(user_input)
            
            # Generate prompt and get LLM response
            prompt = self.get_prompt(user_input, context)
            from langchain.schema import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Update state with results
            state["final_result"] = response.content
            state["errors"] = None
            
            # Add metadata about the request type
            if "metadata" not in state:
                state["metadata"] = {}
            state["metadata"]["detected_request_type"] = request_type
            state["metadata"]["agent_used"] = self.get_agent_name()

        except Exception as e:
            state["final_result"] = None
            if "errors" not in state:
                state["errors"] = []
            state["errors"].append(f"Error in {self.get_agent_name()}: {str(e)}")
            
            # Add error metadata
            if "metadata" not in state:
                state["metadata"] = {}
            state["metadata"]["error_occurred"] = True

        return state
